{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import time\n",
    "import itertools\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import types\n",
    "from pyspark.sql.functions import isnan, when, count, col, rand\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler, StandardScaler, VectorIndexer, Normalizer, SQLTransformer, RFormula\n",
    "from pyspark.sql.functions import udf, avg, col, struct\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.ml.linalg import VectorUDT\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "sc.setCheckpointDir(\"/data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "In the absence of information about what each of these columns are, we can run test Logistic Regressions against each of them to get a sense of whether and how much they affect our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, daypart: int, I1Scaled: double, I2Scaled: double, I3Scaled: double, I4Scaled: double, I5Scaled: double, I6Scaled: double, I7Scaled: double, I8Scaled: double, I9Scaled: double, I10Scaled: double, I11Scaled: double, I12Scaled: double, I13Scaled: double, I1BlankInd: int, I2BlankInd: int, I3BlankInd: int, I4BlankInd: int, I5BlankInd: int, I6BlankInd: int, I7BlankInd: int, I8BlankInd: int, I9BlankInd: int, I10BlankInd: int, I11BlankInd: int, I12BlankInd: int, I13BlankInd: int, C1: string, C2: string, C3: string, C4: string, C5: string, C6: string, C7: string, C8: string, C9: string, C10: string, C11: string, C12: string, C13: string, C14: string, C15: string, C16: string, C17: string, C18: string, C19: string, C20: string, C21: string, C22: string, C22BlankInd: int, C23: string, C24: string, C25: string, C26: string]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pull the balanced train/test data from the bucket\n",
    "trainset = \"gs://w261-final-hoky/data/cross_validation_data_set/balanced_set_1/train/\"\n",
    "testset = \"gs://w261-final-hoky/data/cross_validation_data_set/balanced_set_1/test/\"\n",
    "train_df = spark.read.option(\"header\", \"false\").parquet(trainset)\n",
    "test_df = spark.read.option(\"header\", \"false\").parquet(testset)\n",
    "train_df = train_df.sample(.1, False, seed=261)\n",
    "test_df = test_df.sample(.1, False, seed=261)\n",
    "train_df.cache()\n",
    "test_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "187806"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label=0, daypart=2, I1Scaled=0.0, I2Scaled=-0.276854459264811, I3Scaled=0.0, I4Scaled=0.0, I5Scaled=-0.26700788528012814, I6Scaled=0.0, I7Scaled=0.0, I8Scaled=-0.7568641453123016, I9Scaled=0.0, I10Scaled=0.0, I11Scaled=0.0, I12Scaled=0.0, I13Scaled=0.0, I1BlankInd=1, I2BlankInd=0, I3BlankInd=1, I4BlankInd=1, I5BlankInd=0, I6BlankInd=1, I7BlankInd=1, I8BlankInd=0, I9BlankInd=1, I10BlankInd=1, I11lankInd=1, I12BlankInd=1, I13BlankInd=1, C1='05db9164', C2='bc6e3dc1', C3='67799c69', C4='d00d0f35', C5='25c83c98', C6='fe6b92e5', C7='b1ff5115', C8='0b153874', C9='a73ee510', C10='3b08e48b', C11='c708d1a1', C12='b9f28c33', C13='86dc4b63', C14='cfef1c29', C15='8b9021f6', C16='0f655650', C17='776ce399', C18='3a2028fd', C19='C19_no_value', C20='C20_no_value', C21='b426bc93', C22='C22_no_value', C22BlankInd=1, C23='32c7478e', C24='2e0a0035', C25='C25_no_value', C26='C26_no_value')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create a series of functions that will allow us to run regressions with various column inputs and report back scores for accuracy, log loss, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runRegression(num_feats, cat_feats, train, test):\n",
    "    ''' Runs features through a pipeline into a Logistic Regression\n",
    "    Args:\n",
    "        num_feats - list of column names for numeric features\n",
    "        cat_feats - list of column names for categorical features\n",
    "        train - dataframe for training\n",
    "        test - dataframe for testing\n",
    "    Output:\n",
    "        predictions - test dataframe with column of predictions from the logistic regression\n",
    "    '''\n",
    "    # Pipeline step 1: one hot encoding for the categorical variables\n",
    "    stages = []\n",
    "    for c in cat_feats:\n",
    "        # cast each record in in categorical column c to an index\n",
    "        stridx = StringIndexer(inputCol=c, outputCol = c + \"idx\").setHandleInvalid(\"keep\")\n",
    "        # one hot encode the indexed categorical column\n",
    "        encoder = OneHotEncoderEstimator(inputCols=[stridx.getOutputCol()], outputCols=[c + \"classVec\"]).setDropLast(False)\n",
    "        stages += [stridx, encoder]\n",
    "\n",
    "    # Pipeline step 3: index the label column\n",
    "    label_stridx = StringIndexer(inputCol=\"label\", outputCol=\"label_transformed\").setHandleInvalid('skip')\n",
    "    stages += [label_stridx]\n",
    "\n",
    "    # Pipeline step 4: put all features into one column as type of vector\n",
    "    assembler_inputs = [c + \"classVec\" for c in cat_feats] + [n for n in num_feats]\n",
    "    assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")\n",
    "    stages += [assembler]\n",
    "\n",
    "    # Add the model to the pipeline\n",
    "    lr = LogisticRegression(featuresCol='features', labelCol ='label_transformed', maxIter=4, regParam=.05)\n",
    "    stages += [lr]\n",
    "\n",
    "    # fit the pipeline to do the series of fit/transform defined in stages\n",
    "    pipeline = Pipeline(stages=stages)\n",
    "\n",
    "    # Train the model\n",
    "    pipelineModel = pipeline.fit(train)\n",
    "\n",
    "    # Make predictions\n",
    "    predictions_df = pipelineModel.transform(test)\n",
    "    \n",
    "    #evaluator = MulticlassClassificationEvaluator(labelCol='label_transformed', metricName='accuracy')\n",
    "    #accuracy = evaluator.evaluate(predictions_df)\n",
    "       \n",
    "    return predictions_df\n",
    "\n",
    "def computeLogLoss(prob, label):\n",
    "    ''' Calculates the log loss for a single observation\n",
    "    Args:\n",
    "        prob- float, a probability between 0 and 1\n",
    "        label- integer, a label that is either 0 or 1\n",
    "    Output:\n",
    "        logloss- float, the log loss value for the single observation\n",
    "    '''\n",
    "    prob = prob[int(label)]\n",
    "    # for the special case when prob=0 or 1, need a small value to avoid log(0)\n",
    "    eps = 10e-14\n",
    "    if prob == 0:\n",
    "        prob += eps\n",
    "    if prob == 1:\n",
    "        prob -= eps\n",
    "    try:\n",
    "        logloss = -label * np.log(prob) - (1 - label) * np.log(1-prob)\n",
    "    except Exception as e:\n",
    "        logloss = prob\n",
    "    return logloss\n",
    "\n",
    "def returnMetrics(predictions_df):\n",
    "    # Score predictions\n",
    "    loglossUDF = udf(lambda x: float(computeLogLoss(x[0], x[1])), returnType=FloatType())\n",
    "    newdf = predictions_df.withColumn('logloss', loglossUDF(struct('probability', 'label_transformed')))\n",
    "    logloss = newdf.select(avg(col(\"logloss\"))).collect()[0]['avg(logloss)']\n",
    "    \n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol='label_transformed', metricName='accuracy')\n",
    "    accuracy = evaluator.evaluate(predictions_df)\n",
    "    return (accuracy, logloss)\n",
    "\n",
    "def printMetrics(predictions_and_labels):\n",
    "    metrics = MulticlassMetrics(predictions_and_labels)\n",
    "    print('Precision of True ', metrics.precision(1))\n",
    "    print('Precision of False', metrics.precision(0))\n",
    "    print('Recall of True    ', metrics.recall(1))\n",
    "    print('Recall of False   ', metrics.recall(0))\n",
    "    #print('Weighted F-1 Score', metrics.weightedFMeasure())\n",
    "    print('Average F-1 Score ', (metrics.fMeasure(label=0.0)+metrics.fMeasure(label=1.0))/2)\n",
    "    print('Confusion Matrix\\n', metrics.confusionMatrix().toArray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these functions in place, we can get a baseline of what kind of accuracy we would get if we fed ALL the data through the logistic regression.  We'd also learn how long such a task would take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6851745996536162\n",
      "Log Loss:  0.9483030295010226\n",
      "... Single regression run in 1567.0638630390167 seconds\n"
     ]
    }
   ],
   "source": [
    "# Test a single regression run with ALL columns on a subset of 10% of data:\n",
    "cat_feats = ['C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14', 'C15', 'C16', 'C17', 'C18', 'C19', 'C20', 'C21', 'C22', 'C23', 'C24', 'C25', 'C26']\n",
    "num_feats = ['daypart', 'I1Scaled', 'I2Scaled', 'I3Scaled', 'I4Scaled', 'I5Scaled', 'I6Scaled', 'I7Scaled', 'I8Scaled', 'I9Scaled', 'I10Scaled', 'I11Scaled', 'I12Scaled']\n",
    "num_feats += ['I1BlankInd', 'I2BlankInd', 'I3BlankInd', 'I4BlankInd', 'I5BlankInd', 'I6BlankInd', 'I7BlankInd', 'I8BlankInd', 'I9BlankInd', 'I10BlankInd', 'I11BlankInd', 'I12BlankInd']\n",
    "start = time.time()\n",
    "predictions_df = runRegression(num_feats, cat_feats, train_df, test_df)\n",
    "score = returnMetrics(predictions_df)\n",
    "print(\"Accuracy:\", score[0])\n",
    "print(\"Log Loss: \", score[1])\n",
    "print(f\"... Single regression run in {time.time() - start} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That takes a very long time, and I can't even run it on the full dataset without breaking Spark.  Clearly we need to prune down the list of features, but which ones to use?  We have no idea what any of these columns are.  Using the data from the EDA, we can hand-pick a set of columns that look like they add value:  categorical columns that are not too homogenous, empty, or contain too many unique values, and numerical columns that don't correlate too strongly with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5786332029913465\n",
      "Log Loss:  1.1673509932195787\n",
      "... Single regression run in 110.77921342849731 seconds\n"
     ]
    }
   ],
   "source": [
    "# Test a single regression run with HANDPICKED columns on 100% of data:\n",
    "cat_feats = ['daypart', \"C1\", \"C5\", \"C6\", \"C8\", \"C9\", \"C10\", \"C14\", \"C17\", \"C19\", \"C20\", \"C22\", \"C23\", \"C25\"]\n",
    "num_feats = ['I1Scaled', 'I2Scaled', 'I3Scaled', 'I4Scaled', 'I5Scaled', 'I6Scaled', 'I7Scaled', 'I8Scaled', 'I9Scaled', 'I10Scaled', 'I11Scaled', 'I12Scaled']\n",
    "num_feats += ['I1BlankInd', 'I2BlankInd', 'I3BlankInd', 'I4BlankInd', 'I5BlankInd', 'I6BlankInd', 'I7BlankInd', 'I8BlankInd', 'I9BlankInd', 'I10BlankInd', 'I11lankInd', 'I12BlankInd']\n",
    "start = time.time()\n",
    "predictions_df = runRegression(num_feats, cat_feats, train_df, test_df)\n",
    "score = returnMetrics(predictions_df)\n",
    "print(\"Accuracy:\", score[0])\n",
    "print(\"Log Loss: \", score[1])\n",
    "print(f\"... Single regression run in {time.time() - start} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is much better, but can we improve it even further?  Why not build a set of functions to loop through all the features, and eliminate the ones that suppress accuracy one by one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catfilter(cats, nums, train_df, test_df, score_to_beat):\n",
    "    removal_list = []\n",
    "    winner_cats = cats\n",
    "    current_score = score_to_beat\n",
    "    score_list = []\n",
    "    while True:\n",
    "        print(f'Beginning round with cat features: {winner_cats}')\n",
    "        for c in winner_cats:\n",
    "            trial_cats = winner_cats.copy()\n",
    "            trial_cats.remove(c)\n",
    "            score = runRegression(nums, trial_cats, train_df, test_df)\n",
    "            print(f'{c}: {score}')\n",
    "            score_list.append(score) # Accuracy\n",
    "        # Identify the index of the lowest score\n",
    "        best = score_list.index(max(score_list))\n",
    "        if score_list[best] > current_score:\n",
    "            print(f'The best score of {score_list[best]} comes from removing {winner_cats[best]}.')\n",
    "            current_score = score_list[best]\n",
    "            removal_list.append(winner_cats[best])\n",
    "            del winner_cats[best]\n",
    "            score_list = []\n",
    "        else:\n",
    "            return (winner_cats, removal_list)\n",
    "            break\n",
    "            \n",
    "def numfilter(cats, nums, df_train, df_test, score_to_beat):\n",
    "    removal_list = []\n",
    "    winner_nums = nums\n",
    "    current_score = score_to_beat\n",
    "    score_list = []\n",
    "    while True:\n",
    "        print(f'Beginning round with num features: {winner_nums}')\n",
    "        for c in winner_nums:\n",
    "            trial_nums = winner_nums.copy()\n",
    "            trial_nums.remove(c)\n",
    "            score = runRegression(trial_nums, cats, df_train, df_test)\n",
    "            print(f'{c}: {score}')\n",
    "            score_list.append(score)\n",
    "        # Identify the index of the lowest score\n",
    "        best = score_list.index(max(score_list))\n",
    "        if score_list[best] > current_score:\n",
    "            print(f'The best score of {score_list[best]} comes from removing {winner_nums[best]}.')\n",
    "            current_score = score_list[best]\n",
    "            removal_list.append(winner_nums[best])\n",
    "            del winner_nums[best]\n",
    "            score_list = []\n",
    "        else:\n",
    "            return (winner_nums, removal_list)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning round with num features: ['I1Scaled', 'I2Scaled', 'I3Scaled', 'I4Scaled', 'I5Scaled', 'I6Scaled', 'I7Scaled', 'I8Scaled', 'I9Scaled', 'I10Scaled', 'I11Scaled', 'I12Scaled', 'I13Scaled', 'I1BlankInd', 'I2BlankInd', 'I3BlankInd', 'I4BlankInd', 'I5BlankInd', 'I6BlankInd', 'I7BlankInd', 'I8BlankInd', 'I9BlankInd', 'I10BlankInd', 'I11lankInd', 'I12BlankInd', 'I13BlankInd', 'C22BlankInd']\n",
      "I1Scaled: 0.6380278982333967\n",
      "I2Scaled: 0.6385192147480401\n",
      "I3Scaled: 0.6530023711362228\n",
      "I4Scaled: 0.6405272039818\n",
      "I5Scaled: 0.6397581868284451\n",
      "I6Scaled: 0.6397154636532587\n",
      "I7Scaled: 0.6387114690363788\n",
      "I8Scaled: 0.6534082413004935\n",
      "I9Scaled: 0.6395018477773268\n",
      "I10Scaled: 0.6613333902975669\n",
      "I11Scaled: 0.6386473842735992\n",
      "I12Scaled: 0.6527246704975114\n",
      "I13Scaled: 0.6394377630145472\n",
      "I1BlankInd: 0.6453121996026745\n",
      "I2BlankInd: 0.6530023711362228\n",
      "I3BlankInd: 0.6392882319013949\n",
      "I4BlankInd: 0.6392668703138017\n",
      "I5BlankInd: 0.6529596479610364\n",
      "I6BlankInd: 0.6496913250592784\n",
      "I7BlankInd: 0.6527673936726978\n",
      "I8BlankInd: 0.6529810095486297\n",
      "I9BlankInd: 0.6527673936726978\n",
      "I10BlankInd: 0.6453121996026745\n",
      "I11lankInd: 0.6527673936726978\n",
      "I12BlankInd: 0.6468929570845705\n",
      "I13BlankInd: 0.6392668703138017\n",
      "C22BlankInd: 0.6471065729605024\n",
      "The best score of 0.6613333902975669 comes from removing I10Scaled.\n",
      "Beginning round with num features: ['I1Scaled', 'I2Scaled', 'I3Scaled', 'I4Scaled', 'I5Scaled', 'I6Scaled', 'I7Scaled', 'I8Scaled', 'I9Scaled', 'I11Scaled', 'I12Scaled', 'I13Scaled', 'I1BlankInd', 'I2BlankInd', 'I3BlankInd', 'I4BlankInd', 'I5BlankInd', 'I6BlankInd', 'I7BlankInd', 'I8BlankInd', 'I9BlankInd', 'I10BlankInd', 'I11lankInd', 'I12BlankInd', 'I13BlankInd', 'C22BlankInd']\n",
      "I1Scaled: 0.6610984128340418\n",
      "I2Scaled: 0.6609061585457031\n",
      "I3Scaled: 0.6612693055347874\n",
      "I4Scaled: 0.6610343280712623\n",
      "I5Scaled: 0.6608420737829236\n",
      "I6Scaled: 0.6599021639288232\n",
      "I7Scaled: 0.6616110909362783\n",
      "I8Scaled: 0.6614188366479397\n",
      "I9Scaled: 0.6615042829983124\n",
      "I11Scaled: 0.660436203618653\n",
      "I12Scaled: 0.6614829214107193\n",
      "I13Scaled: 0.6611838591844146\n",
      "I1BlankInd: 0.6641103966846816\n",
      "I2BlankInd: 0.6613333902975669\n",
      "I3BlankInd: 0.6607139042573644\n",
      "I4BlankInd: 0.661226582359601\n",
      "I5BlankInd: 0.6612906671223805\n",
      "I6BlankInd: 0.6652425608271206\n",
      "I7BlankInd: 0.6613761134727533\n",
      "I8BlankInd: 0.6613761134727533\n",
      "I9BlankInd: 0.6613761134727533\n",
      "I10BlankInd: 0.6641103966846816\n",
      "I11lankInd: 0.6613761134727533\n",
      "I12BlankInd: 0.6643026509730203\n",
      "I13BlankInd: 0.661226582359601\n",
      "C22BlankInd: 0.6649221370132228\n",
      "The best score of 0.6652425608271206 comes from removing I6BlankInd.\n",
      "Beginning round with num features: ['I1Scaled', 'I2Scaled', 'I3Scaled', 'I4Scaled', 'I5Scaled', 'I6Scaled', 'I7Scaled', 'I8Scaled', 'I9Scaled', 'I11Scaled', 'I12Scaled', 'I13Scaled', 'I1BlankInd', 'I2BlankInd', 'I3BlankInd', 'I4BlankInd', 'I5BlankInd', 'I7BlankInd', 'I8BlankInd', 'I9BlankInd', 'I10BlankInd', 'I11lankInd', 'I12BlankInd', 'I13BlankInd', 'C22BlankInd']\n",
      "I1Scaled: 0.6652639224147139\n",
      "I2Scaled: 0.6646230747869182\n",
      "I3Scaled: 0.6652425608271206\n",
      "I4Scaled: 0.6653066455899003\n",
      "I5Scaled: 0.6648580522504433\n",
      "I6Scaled: 0.6641744814474612\n",
      "I7Scaled: 0.6656697925789845\n",
      "I8Scaled: 0.6650930297139683\n",
      "I9Scaled: 0.665285284002307\n",
      "I11Scaled: 0.6638967808087497\n",
      "I12Scaled: 0.6651571144767479\n",
      "I13Scaled: 0.6654561767030526\n",
      "I1BlankInd: 0.6616324525238716\n",
      "I2BlankInd: 0.6652425608271206\n",
      "I3BlankInd: 0.6658834084549163\n",
      "I4BlankInd: 0.6655416230534253\n",
      "I5BlankInd: 0.6653066455899003\n",
      "I7BlankInd: 0.6648153290752569\n",
      "I8BlankInd: 0.6652425608271206\n",
      "I9BlankInd: 0.6648153290752569\n",
      "I10BlankInd: 0.6616324525238716\n",
      "I11lankInd: 0.6648153290752569\n",
      "I12BlankInd: 0.6616538141114647\n",
      "I13BlankInd: 0.6655416230534253\n",
      "C22BlankInd: 0.6619101531625831\n",
      "The best score of 0.6658834084549163 comes from removing I3BlankInd.\n",
      "Beginning round with num features: ['I1Scaled', 'I2Scaled', 'I3Scaled', 'I4Scaled', 'I5Scaled', 'I6Scaled', 'I7Scaled', 'I8Scaled', 'I9Scaled', 'I11Scaled', 'I12Scaled', 'I13Scaled', 'I1BlankInd', 'I2BlankInd', 'I4BlankInd', 'I5BlankInd', 'I7BlankInd', 'I8BlankInd', 'I9BlankInd', 'I10BlankInd', 'I11lankInd', 'I12BlankInd', 'I13BlankInd', 'C22BlankInd']\n",
      "I1Scaled: 0.6655416230534253\n",
      "I2Scaled: 0.6650289449511888\n",
      "I3Scaled: 0.6658620468673232\n",
      "I4Scaled: 0.6652211992395275\n",
      "I5Scaled: 0.6593894858265866\n",
      "I6Scaled: 0.659090423600282\n",
      "I7Scaled: 0.6591758699506547\n",
      "I8Scaled: 0.6654134535278662\n",
      "I9Scaled: 0.6659047700425096\n",
      "I11Scaled: 0.6594962937645525\n",
      "I12Scaled: 0.66584068527973\n",
      "I13Scaled: 0.6654775382906457\n",
      "I1BlankInd: 0.6620596842757354\n",
      "I2BlankInd: 0.6658834084549163\n",
      "I4BlankInd: 0.6595176553521458\n",
      "I5BlankInd: 0.6658193236921368\n",
      "I7BlankInd: 0.6658620468673232\n",
      "I8BlankInd: 0.6658834084549163\n",
      "I9BlankInd: 0.6658620468673232\n",
      "I10BlankInd: 0.6620596842757354\n",
      "I11lankInd: 0.6658620468673232\n",
      "I12BlankInd: 0.6618674299873967\n",
      "I13BlankInd: 0.6595176553521458\n",
      "C22BlankInd: 0.6619955995129558\n",
      "The best score of 0.6659047700425096 comes from removing I9Scaled.\n",
      "Beginning round with num features: ['I1Scaled', 'I2Scaled', 'I3Scaled', 'I4Scaled', 'I5Scaled', 'I6Scaled', 'I7Scaled', 'I8Scaled', 'I11Scaled', 'I12Scaled', 'I13Scaled', 'I1BlankInd', 'I2BlankInd', 'I4BlankInd', 'I5BlankInd', 'I7BlankInd', 'I8BlankInd', 'I9BlankInd', 'I10BlankInd', 'I11lankInd', 'I12BlankInd', 'I13BlankInd', 'C22BlankInd']\n",
      "I1Scaled: 0.6656270694037981\n",
      "I2Scaled: 0.6652425608271206\n",
      "I3Scaled: 0.6658834084549163\n",
      "I4Scaled: 0.6652211992395275\n",
      "I5Scaled: 0.6593254010638071\n",
      "I6Scaled: 0.658983615662316\n",
      "I7Scaled: 0.6591331467754683\n",
      "I8Scaled: 0.665498899878239\n",
      "I11Scaled: 0.6594108474141798\n",
      "I12Scaled: 0.6658193236921368\n",
      "I13Scaled: 0.6657766005169504\n",
      "I1BlankInd: 0.6621878538012945\n",
      "I2BlankInd: 0.6659047700425096\n",
      "I4BlankInd: 0.6596244632901117\n",
      "I5BlankInd: 0.66584068527973\n",
      "I7BlankInd: 0.6656911541665776\n",
      "I8BlankInd: 0.6659261316301027\n",
      "I9BlankInd: 0.6656911541665776\n",
      "I10BlankInd: 0.6621878538012945\n",
      "I11lankInd: 0.6656911541665776\n",
      "I12BlankInd: 0.6616965372866511\n",
      "I13BlankInd: 0.6596244632901117\n",
      "C22BlankInd: 0.6619528763377694\n",
      "The best score of 0.6659261316301027 comes from removing I8BlankInd.\n",
      "Beginning round with num features: ['I1Scaled', 'I2Scaled', 'I3Scaled', 'I4Scaled', 'I5Scaled', 'I6Scaled', 'I7Scaled', 'I8Scaled', 'I11Scaled', 'I12Scaled', 'I13Scaled', 'I1BlankInd', 'I2BlankInd', 'I4BlankInd', 'I5BlankInd', 'I7BlankInd', 'I9BlankInd', 'I10BlankInd', 'I11lankInd', 'I12BlankInd', 'I13BlankInd', 'C22BlankInd']\n",
      "I1Scaled: 0.6656270694037981\n",
      "I2Scaled: 0.6652639224147139\n",
      "I3Scaled: 0.6659047700425096\n",
      "I4Scaled: 0.6652211992395275\n",
      "I5Scaled: 0.6593254010638071\n",
      "I6Scaled: 0.658983615662316\n",
      "I7Scaled: 0.6591331467754683\n",
      "I8Scaled: 0.6654775382906457\n",
      "I11Scaled: 0.6593681242389935\n",
      "I12Scaled: 0.6658193236921368\n",
      "I13Scaled: 0.665733877341764\n",
      "I1BlankInd: 0.6622092153888877\n",
      "I2BlankInd: 0.6659261316301027\n",
      "I4BlankInd: 0.6596031017025186\n",
      "I5BlankInd: 0.6658193236921368\n",
      "I7BlankInd: 0.6656911541665776\n",
      "I9BlankInd: 0.6656911541665776\n",
      "I10BlankInd: 0.6622092153888877\n",
      "I11lankInd: 0.6656911541665776\n",
      "I12BlankInd: 0.6617178988742444\n",
      "I13BlankInd: 0.6596031017025186\n",
      "C22BlankInd: 0.6619528763377694\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['I1Scaled',\n",
       "  'I2Scaled',\n",
       "  'I3Scaled',\n",
       "  'I4Scaled',\n",
       "  'I5Scaled',\n",
       "  'I6Scaled',\n",
       "  'I7Scaled',\n",
       "  'I8Scaled',\n",
       "  'I11Scaled',\n",
       "  'I12Scaled',\n",
       "  'I13Scaled',\n",
       "  'I1BlankInd',\n",
       "  'I2BlankInd',\n",
       "  'I4BlankInd',\n",
       "  'I5BlankInd',\n",
       "  'I7BlankInd',\n",
       "  'I9BlankInd',\n",
       "  'I10BlankInd',\n",
       "  'I11lankInd',\n",
       "  'I12BlankInd',\n",
       "  'I13BlankInd',\n",
       "  'C22BlankInd'],\n",
       " ['I10Scaled', 'I6BlankInd', 'I3BlankInd', 'I9Scaled', 'I8BlankInd'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_feats = ['I1Scaled', 'I2Scaled', 'I3Scaled', 'I4Scaled', 'I5Scaled', 'I6Scaled', 'I7Scaled', \n",
    "             'I8Scaled', 'I9Scaled', 'I10Scaled', 'I11Scaled', 'I12Scaled', 'I13Scaled']\n",
    "num_feats += ['I1BlankInd', 'I2BlankInd', 'I3BlankInd', 'I4BlankInd', 'I5BlankInd', 'I6BlankInd', 'I7BlankInd', \n",
    "              'I8BlankInd', 'I9BlankInd', 'I10BlankInd', 'I11lankInd', 'I12BlankInd', 'I13BlankInd', 'C22BlankInd']\n",
    "cat_feats = ['C1', 'C2', 'C4', 'C5', 'C6', 'C7', 'C8', 'C10', 'C11', 'C12', 'C13', \n",
    "             'C14', 'C15', 'C17', 'C18', 'C19', 'C20', 'C22', 'C24', 'C26']\n",
    "numfilter(cat_feats, num_feats, train_df, test_df, .3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, we derive that we can trim an additional four columns from our handpicked set and still get essentially the same result.  If we start from the whole array of features and winnow down, we determine that the optimal combination of features are:\n",
    "\n",
    "**Categorical:** ['daypart', 'C1', 'C2', 'C5', 'C6', 'C7', 'C8', 'C11', 'C13', 'C14', 'C15', 'C17', 'C18', 'C19', 'C20', 'C22', 'C26']\n",
    "\n",
    "**Numeric:** ['I1Scaled', 'I2Scaled', 'I3Scaled', 'I4Scaled', 'I5Scaled', 'I6Scaled', 'I7Scaled', 'I8Scaled', 'I11Scaled', 'I12Scaled', 'I13Scaled', 'I1BlankInd', 'I3BlankInd', 'I5BlankInd', 'I6BlankInd', 'I7BlankInd', 'I9BlankInd', 'I10BlankInd', 'I11BlankInd', 'I12BlankInd', 'C22BlankInd']\n",
    "\n",
    "So with an algorithmically-designed set of features, what sort of scores do we see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6910073962852925\n",
      "Log Loss:  0.8567510902837625\n",
      "... Single regression run in 406.1963346004486 seconds\n"
     ]
    }
   ],
   "source": [
    "# Test a single regression run with OPTIMIZED columns on 100% of the data:\n",
    "cat_feats = ['daypart', 'C1', 'C2', 'C5', 'C6', 'C7', 'C8', 'C11', 'C13', \n",
    "             'C14', 'C15', 'C17', 'C18', 'C19', 'C20', 'C22', 'C26']\n",
    "num_feats = ['I1Scaled', 'I2Scaled', 'I3Scaled', 'I4Scaled', 'I5Scaled', 'I6Scaled', 'I7Scaled', 'I8Scaled', \n",
    "             'I11Scaled', 'I12Scaled', 'I13Scaled', 'I1BlankInd', 'I3BlankInd', 'I5BlankInd', 'I6BlankInd', \n",
    "             'I7BlankInd', 'I9BlankInd', 'I10BlankInd', 'I11BlankInd', 'I12BlankInd', 'C22BlankInd']\n",
    "start = time.time()\n",
    "predictions_df = runRegression(num_feats, cat_feats, train_df, test_df)\n",
    "score = returnMetrics(predictions_df)\n",
    "print(\"Accuracy:\", score[0])\n",
    "print(\"Log Loss: \", score[1])\n",
    "print(f\"... Single regression run in {time.time() - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6855601115022973\n",
      "Log Loss:  0.8717612380033435\n",
      "... Single regression run in 177.25854587554932 seconds\n"
     ]
    }
   ],
   "source": [
    "# Test a single regression run with OPTIMIZED columns on 100% of the data:\n",
    "cat_feats = ['daypart', 'C1', 'C2', 'C5', 'C6', 'C7', 'C8', 'C11', 'C13', \n",
    "             'C14', 'C15', 'C17', 'C18', 'C19', 'C20', 'C22', 'C26']\n",
    "num_feats = ['I1Scaled', 'I2Scaled', 'I3Scaled', 'I4Scaled', 'I5Scaled', 'I6Scaled', 'I7Scaled', 'I8Scaled', \n",
    "             'I11Scaled', 'I12Scaled', 'I13Scaled', 'I1BlankInd', 'I3BlankInd', 'I5BlankInd', 'I6BlankInd', \n",
    "             'I7BlankInd', 'I9BlankInd', 'I10BlankInd', 'I11BlankInd', 'I12BlankInd', 'C22BlankInd']\n",
    "start = time.time()\n",
    "predictions_df = runRegression(num_feats, cat_feats, train_df, test_df)\n",
    "score = returnMetrics(predictions_df)\n",
    "print(\"Accuracy:\", score[0])\n",
    "print(\"Log Loss: \", score[1])\n",
    "print(f\"... Single regression run in {time.time() - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision of True  0.7016657830854656\n",
      "Precision of False 0.671935608236229\n",
      "Recall of True     0.6440407386222969\n",
      "Recall of False    0.7269593196966214\n",
      "Weighted F-1 Score 0.685011774438858\n",
      "Average F-1 Score  0.684992394542645\n",
      "Confusion Matrix\n",
      " [[170802.  64152.]\n",
      " [ 83392. 150882.]]\n"
     ]
    }
   ],
   "source": [
    "results_rdd = predictions_df.select([\"prediction\", \"label_transformed\"]).rdd\n",
    "metrics = MulticlassMetrics(results_rdd)\n",
    "print('Precision of True ', metrics.precision(1))\n",
    "print('Precision of False', metrics.precision(0))\n",
    "print('Recall of True    ', metrics.recall(1))\n",
    "print('Recall of False   ', metrics.recall(0))\n",
    "print('Weighted F-1 Score', metrics.weightedFMeasure())\n",
    "print('Average F-1 Score ', (metrics.fMeasure(label=0.0)+metrics.fMeasure(label=1.0))/2)\n",
    "print('Confusion Matrix\\n', metrics.confusionMatrix().toArray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better accuracy than the features we picked by hand, slightly better to what we saw with all columns on a 10% subset of the data, and ran on 100% of the data in 1/10th the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactions\n",
    "These are the highest-value features taken independently, but is there additional information that can be derived by looking at them in combination?  For example, one of the numericals might be number of times this user has visited this site, while one of the categoricals might be the referring URL, and perhaps heavy users of THIS site who just came from THAT site are more likely to click on a particular ad.  In order to examine that, we would need an interaction term.\n",
    "\n",
    "The trick comes in determining which fields to interact.  Interacting everything on everything else would be an exponentiation of features, and most of those combinations would make no sense.  So first we can do an exploration to see whether we can find any instances of this that contribute to accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runRegressionInter(num, cat, train, test, inter=None, interwith=None):\n",
    "    ''' Runs features through a pipeline into a Logistic Regression\n",
    "    Args:\n",
    "        num_feats - list of column names for numeric features\n",
    "        cat_feats - list of column names for categorical features\n",
    "        train - dataframe for training\n",
    "        test - dataframe for testing\n",
    "    Output:\n",
    "        predictions - test dataframe with column of predictions from the logistic regression\n",
    "    '''\n",
    "    # Pipeline step 1: one hot encoding for the categorical variables\n",
    "    stages = []\n",
    "    for c in cat:\n",
    "        # cast each record in in categorical column c to an index\n",
    "        stridx = StringIndexer(inputCol=c, outputCol = c + \"idx\").setHandleInvalid(\"keep\")\n",
    "        # one hot encode the indexed categorical column\n",
    "        encoder = OneHotEncoderEstimator(inputCols=[stridx.getOutputCol()], outputCols=[c + \"classVec\"]).setDropLast(False)\n",
    "        stages += [stridx, encoder]\n",
    "\n",
    "    # Pipeline step 3: index the label column\n",
    "    label_stridx = StringIndexer(inputCol=\"label\", outputCol=\"label_transformed\").setHandleInvalid('skip')\n",
    "    stages += [label_stridx]\n",
    "\n",
    "    # Pipeline step 4: add interactions, and store in a features vector as type of sparse vector\n",
    "\n",
    "    # Get string to indicate all terms we want in our function\n",
    "    assembler_inputs = [c + \"classVec\" for c in cat] + [n for n in num]\n",
    "    formula_string='label ~ ' + (' + ').join(assembler_inputs)\n",
    "    \n",
    "    if inter is not None:\n",
    "        inter = inter+\"classVec\" if inter[0]==\"C\" and inter!=\"C22Bin\" else inter\n",
    "        interwith = interwith+\"classVec\" if interwith[0]==\"C\" and interwith!=\"C22Bin\" else interwith\n",
    "        formula_string += ' + ' + inter + ':' + interwith\n",
    "        \n",
    "    #print(formula_string)\n",
    "    \n",
    "    # Create formula for the function\n",
    "    formula = RFormula(formula=formula_string, featuresCol='features', labelCol='label')\n",
    "    stages +=[formula]\n",
    "\n",
    "    # Add the model to the pipeline\n",
    "    lr = LogisticRegression(featuresCol='features', labelCol ='label_transformed', maxIter=4, regParam=.05, elasticNetParam=0, standardization=True)\n",
    "    stages += [lr]\n",
    "\n",
    "    # fit the pipeline to do the series of fit/transform defined in stages\n",
    "    pipeline = Pipeline(stages=stages)\n",
    "\n",
    "    # Train the model\n",
    "    pipelineModel = pipeline.fit(train)\n",
    "\n",
    "    # Make predictions\n",
    "    predictions_df = pipelineModel.transform(test)\n",
    "\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol='label_transformed', metricName='accuracy')\n",
    "    accuracy = evaluator.evaluate(predictions_df)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline with no interactions: 0.6610770512464487\n",
      "Interacting C5 with C17: 0.6660115779804755, net change: 0.004934526734026834\n",
      "Interacting C6 with C17: 0.6674000811740328, net change: 0.006323029927584156\n",
      "Interacting C8 with C17: 0.6663533633819666, net change: 0.005276312135517935\n",
      "Interacting C14 with C17: 0.6669087646593895, net change: 0.005831713412940864\n",
      "Interacting C17 with C20: 0.6668874030717963, net change: 0.005810351825347615\n",
      "Interacting C17 with C22: 0.6655416230534253, net change: 0.0044645718069766804\n"
     ]
    }
   ],
   "source": [
    "cat_feats = ['daypart', 'C1', 'C2', 'C4', 'C5', 'C6', 'C7', 'C8', 'C10', 'C11', 'C12', 'C13', \n",
    "             'C14', 'C15', 'C17', 'C18', 'C19', 'C20', 'C22', 'C24', 'C26']\n",
    "num_feats = ['I1Scaled', 'I2Scaled', 'I3Scaled', 'I4Scaled', 'I5Scaled', 'I6Scaled', 'I7Scaled', 'I8Scaled', \n",
    "             'I11Scaled', 'I12Scaled', 'I13Scaled']\n",
    "small_feats = ['daypart', 'C5', 'C6', 'C8', 'C14', 'C17', 'C20', 'C22'] + num_feats\n",
    "num_feats += ['I1BlankInd', 'I3BlankInd', 'I5BlankInd', 'I6BlankInd', 'I7BlankInd', 'I9BlankInd', \n",
    "              'I10BlankInd', 'I11lankInd', 'I12BlankInd', 'C22BlankInd']\n",
    "start = time.time()\n",
    "baseline = runRegressionInter(num_feats, cat_feats, train_df, test_df)\n",
    "print(\"Baseline with no interactions:\", baseline)\n",
    "for pair in itertools.combinations([i for i in range(len(small_feats))], 2):\n",
    "    score = runRegressionInter(num_feats, cat_feats, train_df, test_df, small_feats[pair[0]], small_feats[pair[1]])\n",
    "    if score-baseline > .001:\n",
    "        print(f'Interacting {small_feats[pair[0]]} with {small_feats[pair[1]]}: {score}, net change: {score-baseline}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runRegressionWithInteractions(num, cat, train, test, inters=[]):\n",
    "    ''' Runs features through a pipeline into a Logistic Regression\n",
    "    Args:\n",
    "        num_feats - list of column names for numeric features\n",
    "        cat_feats - list of column names for categorical features\n",
    "        train - dataframe for training\n",
    "        test - dataframe for testing\n",
    "    Output:\n",
    "        predictions - test dataframe with column of predictions from the logistic regression\n",
    "    '''\n",
    "    # Pipeline step 1: one hot encoding for the categorical variables\n",
    "    stages = []\n",
    "    for c in cat:\n",
    "        # cast each record in in categorical column c to an index\n",
    "        stridx = StringIndexer(inputCol=c, outputCol = c + \"idx\").setHandleInvalid(\"keep\")\n",
    "        # one hot encode the indexed categorical column\n",
    "        encoder = OneHotEncoderEstimator(inputCols=[stridx.getOutputCol()], outputCols=[c + \"classVec\"]).setDropLast(False)\n",
    "        stages += [stridx, encoder]\n",
    "\n",
    "    # Pipeline step 3: index the label column\n",
    "    label_stridx = StringIndexer(inputCol=\"label\", outputCol=\"label_transformed\").setHandleInvalid('skip')\n",
    "    stages += [label_stridx]\n",
    "\n",
    "    # Pipeline step 4: add interactions, and store in a features vector as type of sparse vector\n",
    "\n",
    "    # Get string to indicate all terms we want in our function\n",
    "    assembler_inputs = [c + \"classVec\" for c in cat] + [n for n in num]\n",
    "    formula_string='label ~ ' + (' + ').join(assembler_inputs)\n",
    "    \n",
    "    if inters:\n",
    "        for inter in inters:\n",
    "            i1 = inter[0]+\"classVec\" if inter[0][0]==\"C\" and inter[0]!=\"C22Bin\" else inter[0]\n",
    "            i2 = inter[1]+\"classVec\" if inter[1][0]==\"C\" and inter[1]!=\"C22Bin\" else inter[1]\n",
    "            formula_string += ' + ' + i1 + ':' + i2\n",
    "        \n",
    "    #print(formula_string)\n",
    "    \n",
    "    # Create formula for the function\n",
    "    formula = RFormula(formula=formula_string, featuresCol='features', labelCol='label')\n",
    "    stages +=[formula]\n",
    "\n",
    "    # Add the model to the pipeline\n",
    "    lr = LogisticRegression(featuresCol='features', labelCol ='label_transformed', maxIter=4, regParam=.05, elasticNetParam=0, standardization=True)\n",
    "    stages += [lr]\n",
    "\n",
    "    # fit the pipeline to do the series of fit/transform defined in stages\n",
    "    pipeline = Pipeline(stages=stages)\n",
    "\n",
    "    # Train the model\n",
    "    pipelineModel = pipeline.fit(train)\n",
    "\n",
    "    # Make predictions\n",
    "    predictions_df = pipelineModel.transform(test)\n",
    "   \n",
    "    return predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6908240349089844\n",
      "Log Loss:  0.8542615411319706\n",
      "... Single regression run in 346.16791677474976 seconds\n"
     ]
    }
   ],
   "source": [
    "# Test a single regression run with OPTIMIZED columns and INTERACTIONS on 100% of the data:\n",
    "cat_feats = ['daypart', 'C1', 'C2', 'C5', 'C6', 'C7', 'C8', 'C11', 'C13', \n",
    "             'C14', 'C15', 'C17', 'C18', 'C19', 'C20', 'C22', 'C26']\n",
    "num_feats = ['I1Scaled', 'I2Scaled', 'I3Scaled', 'I4Scaled', 'I5Scaled', 'I6Scaled', 'I7Scaled', 'I8Scaled', \n",
    "             'I11Scaled', 'I12Scaled', 'I13Scaled', 'I1BlankInd', 'I3BlankInd', 'I5BlankInd', 'I6BlankInd', \n",
    "             'I7BlankInd', 'I9BlankInd', 'I10BlankInd', 'I11lankInd', 'I12BlankInd', 'C22BlankInd']\n",
    "interactions = [('C6', 'C17')]\n",
    "start = time.time()\n",
    "predictions_df = runRegressionWithInteractions(num_feats, cat_feats, train_df, test_df, interactions)\n",
    "score = returnMetrics(predictions_df)\n",
    "print(\"Accuracy:\", score[0])\n",
    "print(\"Log Loss: \", score[1])\n",
    "print(f\"... Single regression run in {time.time() - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions = [('C17', 'I1Scaled'), ('C17', 'I11Scaled')]\n",
    "for inter in range(len(interactions)):\n",
    "    newlist = interactions.copy()\n",
    "    del newlist[inter]\n",
    "    predictions_df = runRegressionWithInteractions(num_feats, cat_feats, train_df, test_df, newlist)\n",
    "    score = returnMetrics(predictions_df)\n",
    "    print(f'Without {interactions[inter]}, score is: {score[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for set 1: 0.7670514045618169\n",
      "Log Loss for set 1: 1.6144608416904505\n",
      "\n",
      "Accuracy for set 2: 0.7673359297507747\n",
      "Log Loss for set 2: 1.614008899716377\n",
      "\n",
      "Accuracy for set 3: 0.7673559240105644\n",
      "Log Loss for set 3: 1.6150568149390079\n",
      "\n",
      "Accuracy for set 4: 0.7671941681527592\n",
      "Log Loss for set 4: 1.6143941711082783\n",
      "\n",
      "Accuracy for set 5: 0.7670995347960388\n",
      "Log Loss for set 5: 1.6140322826341726\n",
      "\n",
      "Precision of True  0.6591472606579001\n",
      "Precision of False 0.7757394945551667\n",
      "Recall of True     0.1883584041300187\n",
      "Recall of False    0.9664702416615917\n",
      "Average F-1 Score  0.5768279967541731\n",
      "Confusion Matrix\n",
      " [[32951915.  1143201.]\n",
      " [ 9526153.  2210743.]]\n",
      "... Total Cross-Validated run in 2256.473196029663 seconds\n"
     ]
    }
   ],
   "source": [
    "# Run Cross-Validated model evaluation with UNBALANCED training data on UNBALANCED test\n",
    "cat_feats = ['daypart', 'C1', 'C2', 'C5', 'C6', 'C7', 'C8', 'C11', 'C13', \n",
    "             'C14', 'C15', 'C17', 'C18', 'C19', 'C20', 'C22', 'C26']\n",
    "num_feats = ['I1Scaled', 'I2Scaled', 'I3Scaled', 'I4Scaled', 'I5Scaled', 'I6Scaled', 'I7Scaled', 'I8Scaled', \n",
    "             'I11Scaled', 'I12Scaled', 'I13Scaled', 'I1BlankInd', 'I3BlankInd', 'I5BlankInd', 'I6BlankInd', \n",
    "             'I7BlankInd', 'I9BlankInd', 'I10BlankInd', 'I11BlankInd', 'I12BlankInd', 'C22BlankInd']\n",
    "interactions = [('C6', 'C17')]\n",
    "results = []\n",
    "start = time.time()\n",
    "for CV_set in range(1, 6):\n",
    "    trainset = \"gs://w261-final-hoky/data/cross_validation_data_set/set_\"+str(CV_set)+\"/train/\"\n",
    "    testset = \"gs://w261-final-hoky/data/cross_validation_data_set/set_\"+str(CV_set)+\"/test/\"\n",
    "    train_df = spark.read.option(\"header\", \"false\").parquet(trainset)\n",
    "    test_df = spark.read.option(\"header\", \"false\").parquet(testset)\n",
    "    train_df.cache()\n",
    "    test_df.cache()\n",
    "    \n",
    "    predictions_df = runRegressionWithInteractions(num_feats, cat_feats, train_df, test_df, interactions)\n",
    "    score = returnMetrics(predictions_df)\n",
    "    print(f\"Accuracy for set {CV_set}: {score[0]}\")\n",
    "    print(f\"Log Loss for set {CV_set}: {score[1]}\\n\")\n",
    "    \n",
    "    results = results.union(predictions_df) if results else predictions_df\n",
    "\n",
    "# Convert to RDD to print complete metrics\n",
    "results_rdd = results.select([\"prediction\", \"label_transformed\"]).rdd\n",
    "printMetrics(results_rdd)\n",
    "print(f\"... Total Cross-Validated run in {time.time() - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for set 1: 0.6910082479503615\n",
      "Log Loss for set 1: 0.8536549623519875\n",
      "\n",
      "Accuracy for set 2: 0.6909831978966476\n",
      "Log Loss for set 2: 0.8538127788145157\n",
      "\n",
      "Accuracy for set 3: 0.6904710851853004\n",
      "Log Loss for set 3: 0.8550057711334526\n",
      "\n",
      "Accuracy for set 4: 0.6906207365968081\n",
      "Log Loss for set 4: 0.8522126511312598\n",
      "\n",
      "Accuracy for set 5: 0.6905541717786448\n",
      "Log Loss for set 5: 0.8530239266314799\n",
      "\n",
      "Precision of True  0.7069914704921786\n",
      "Precision of False 0.6768331871934024\n",
      "Recall of True     0.6514409772396381\n",
      "Recall of False    0.7300140514152975\n",
      "F-1 Score          0.6907275143274678\n",
      "Confusion Matrix\n",
      " [[8568099. 3168797.]\n",
      " [4091001. 7645895.]]\n",
      "... Total Cross-Validated run in 1480.1901302337646 seconds\n"
     ]
    }
   ],
   "source": [
    "# Run Cross-Validated model evaluation with BALANCED training data on BALANCED test\n",
    "cat_feats = ['daypart', 'C1', 'C2', 'C5', 'C6', 'C7', 'C8', 'C11', 'C13', \n",
    "             'C14', 'C15', 'C17', 'C18', 'C19', 'C20', 'C22', 'C26']\n",
    "num_feats = ['I1Scaled', 'I2Scaled', 'I3Scaled', 'I4Scaled', 'I5Scaled', 'I6Scaled', 'I7Scaled', 'I8Scaled', \n",
    "             'I11Scaled', 'I12Scaled', 'I13Scaled', 'I1BlankInd', 'I3BlankInd', 'I5BlankInd', 'I6BlankInd', \n",
    "             'I7BlankInd', 'I9BlankInd', 'I10BlankInd', 'I11BlankInd', 'I12BlankInd', 'C22BlankInd']\n",
    "interactions = [('C6', 'C17')]\n",
    "results = []\n",
    "start = time.time()\n",
    "for CV_set in range(1, 6):\n",
    "    trainset = \"gs://w261-final-hoky/data/cross_validation_data_set/balanced_set_\"+str(CV_set)+\"/train/\"\n",
    "    testset = \"gs://w261-final-hoky/data/cross_validation_data_set/balanced_set_\"+str(CV_set)+\"/test/\"\n",
    "    train_df = spark.read.option(\"header\", \"false\").parquet(trainset)\n",
    "    test_df = spark.read.option(\"header\", \"false\").parquet(testset)\n",
    "    train_df.cache()\n",
    "    test_df.cache()\n",
    "    \n",
    "    predictions_df = runRegressionWithInteractions(num_feats, cat_feats, train_df, test_df, interactions)\n",
    "    score = returnMetrics(predictions_df)\n",
    "    print(f\"Accuracy for set {CV_set}: {score[0]}\")\n",
    "    print(f\"Log Loss for set {CV_set}: {score[1]}\\n\")\n",
    "    \n",
    "    results = results.union(predictions_df) if results else predictions_df\n",
    "\n",
    "# Convert to RDD to print complete metrics\n",
    "results_rdd = results.select([\"prediction\", \"label_transformed\"]).rdd\n",
    "printMetrics(results_rdd)\n",
    "print(f\"... Total Cross-Validated run in {time.time() - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for set 1: 0.7083156590806539\n",
      "Log Loss for set 1: 0.9629934231484193\n",
      "\n",
      "Accuracy for set 2: 0.7092321173412258\n",
      "Log Loss for set 2: 0.9628230923929499\n",
      "\n",
      "Accuracy for set 3: 0.7093607588686803\n",
      "Log Loss for set 3: 0.9646067583893108\n",
      "\n",
      "Accuracy for set 4: 0.7073435429573942\n",
      "Log Loss for set 4: 0.9617774045314033\n",
      "\n",
      "Accuracy for set 5: 0.7078682330126151\n",
      "Log Loss for set 5: 0.9619883839616282\n",
      "\n",
      "Precision of True  0.45192762249587815\n",
      "Precision of False 0.8585094851192203\n",
      "Recall of True     0.6514409772396381\n",
      "Recall of False    0.7280399046009992\n",
      "F-1 Score          0.7084240595852523\n",
      "Confusion Matrix\n",
      " [[24822605.  9272511.]\n",
      " [ 4091001.  7645895.]]\n",
      "... Total Cross-Validated run in 1587.3708455562592 seconds\n"
     ]
    }
   ],
   "source": [
    "# Run Cross-Validated model evaluation with BALANCED training data on UNBALANCED test\n",
    "cat_feats = ['daypart', 'C1', 'C2', 'C5', 'C6', 'C7', 'C8', 'C11', 'C13', \n",
    "             'C14', 'C15', 'C17', 'C18', 'C19', 'C20', 'C22', 'C26']\n",
    "num_feats = ['I1Scaled', 'I2Scaled', 'I3Scaled', 'I4Scaled', 'I5Scaled', 'I6Scaled', 'I7Scaled', 'I8Scaled', \n",
    "             'I11Scaled', 'I12Scaled', 'I13Scaled', 'I1BlankInd', 'I3BlankInd', 'I5BlankInd', 'I6BlankInd', \n",
    "             'I7BlankInd', 'I9BlankInd', 'I10BlankInd', 'I11BlankInd', 'I12BlankInd', 'C22BlankInd']\n",
    "interactions = [('C6', 'C17')]\n",
    "results = []\n",
    "start = time.time()\n",
    "for CV_set in range(1, 6):\n",
    "    trainset = \"gs://w261-final-hoky/data/cross_validation_data_set/balanced_set_\"+str(CV_set)+\"/train/\"\n",
    "    testset = \"gs://w261-final-hoky/data/cross_validation_data_set/set_\"+str(CV_set)+\"/test/\"\n",
    "    train_df = spark.read.option(\"header\", \"false\").parquet(trainset)\n",
    "    test_df = spark.read.option(\"header\", \"false\").parquet(testset)\n",
    "    train_df.cache()\n",
    "    test_df.cache()\n",
    "    \n",
    "    predictions_df = runRegressionWithInteractions(num_feats, cat_feats, train_df, test_df, interactions)\n",
    "    score = returnMetrics(predictions_df)\n",
    "    print(f\"Accuracy for set {CV_set}: {score[0]}\")\n",
    "    print(f\"Log Loss for set {CV_set}: {score[1]}\\n\")\n",
    "    \n",
    "    results = results.union(predictions_df) if results else predictions_df\n",
    "\n",
    "# Convert to RDD to print complete metrics\n",
    "results_rdd = results.select([\"prediction\", \"label_transformed\"]).rdd\n",
    "printMetrics(results_rdd)\n",
    "print(f\"... Total Cross-Validated run in {time.time() - start} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
